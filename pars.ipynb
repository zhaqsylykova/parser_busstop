{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc072d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "\n",
    "HEADERS = ['title', 'latitude', 'longitude']  \n",
    "MARKUP_FILENAME = 'html-markup.txt'\n",
    "CSV_DATA_FILENAME=  r'C:\\Users\\Professional\\Downloads\\data69.csv'\n",
    "\n",
    "URL = f'https://2gis.kz/almaty/search/остановки\n",
    "\n",
    "with open(CSV_DATA_FILENAME, 'w', encoding='utf-8-sig', newline='') as f:\n",
    "    csv_writer = csv.DictWriter(f, fieldnames=HEADERS)\n",
    "    csv_writer.writeheader()\n",
    "\n",
    "def extract_url_from_element(element):\n",
    "    # Пытаемся найти тег <a> со ссылкой внутри текущего элемента\n",
    "    link = element.find('a', href=True)\n",
    "    if link and 'href' in link.attrs:\n",
    "        return link['href']\n",
    "    return None\n",
    "\n",
    "def extract_coordinates(script_text):\n",
    "    match = re.search(r'centroid\":\"POINT\\(([\\d.]+) ([\\d.]+)\\)\"', script_text)\n",
    "    if match:\n",
    "        longitude, latitude = match.groups()\n",
    "        return latitude, longitude\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def data_handler(page_number):\n",
    "    with open(MARKUP_FILENAME, 'r', encoding='utf-8-sig') as f:\n",
    "        contents = f.read()\n",
    "    doc = BeautifulSoup(contents, features='html.parser')  \n",
    "    parent_div_results = doc.find_all('div', class_='_1kf6gff')\n",
    "\n",
    "    TITLES = []\n",
    "    LATITUDES = []\n",
    "    LONGITUDES = []\n",
    "    URLS = [] \n",
    "\n",
    "\n",
    "    for idx, res_div in enumerate(parent_div_results):\n",
    "        title = res_div.find('span', class_='_15a9jdw').text if res_div.find('span', class_='_15a9jdw') else 'null'\n",
    "        TITLES.append(title)\n",
    "\n",
    "        stop_url = extract_url_from_element(res_div)\n",
    "        URLS.append(stop_url)\n",
    "\n",
    "        if stop_url:\n",
    "            browser.get('https://2gis.kz/'+ stop_url)\n",
    "            WebDriverWait(browser, 10).until(EC.presence_of_element_located((By.TAG_NAME, \"script\")))\n",
    "            scripts = browser.find_elements(By.TAG_NAME, 'script')\n",
    "            found = False\n",
    "            for script in scripts:\n",
    "                if 'centroid' in script.get_attribute('innerHTML'):\n",
    "                    latitude, longitude = extract_coordinates(script.get_attribute('innerHTML'))\n",
    "                    if latitude and longitude:\n",
    "                        LATITUDES.append(latitude)\n",
    "                        LONGITUDES.append(longitude)\n",
    "                        found = True\n",
    "                        break\n",
    "\n",
    "            if not found:\n",
    "                LATITUDES.append('null')\n",
    "                LONGITUDES.append('null')\n",
    "\n",
    "            browser.back()\n",
    "        else:\n",
    "            LATITUDES.append('null')\n",
    "            LONGITUDES.append('null')\n",
    "\n",
    "               \n",
    "    with open(CSV_DATA_FILENAME, 'a', encoding='utf-8-sig', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "\n",
    "        for m in range(1, len(TITLES)):\n",
    "            writer.writerow([TITLES[m], LATITUDES[m], LONGITUDES[m]])\n",
    "\n",
    "    print(f'\\nfinished parsing page: {page_number}') \n",
    "\n",
    "\n",
    "\n",
    "chrome_binary_path = 'C:/Program Files/Google/Chrome/Application/chrome.exe'\n",
    "chromedriver_path = r'‪C:\\Users\\Professional\\Downloads\\chrome-win64.zip'\n",
    "\n",
    "options = Options()\n",
    "options.binary_location = 'C:/Program Files/Google/Chrome/Application/chrome.exe' \n",
    "browser = webdriver.Chrome(executable_path='C:/Users/Professional/Desktop/chromedriver.exe', chrome_options=options)\n",
    "\n",
    "browser.maximize_window()\n",
    "browser.get(URL)\n",
    "browser.implicitly_wait(40)\n",
    "\n",
    "page_element = browser.find_element(\n",
    "    By.XPATH, \"(//span[@class='_1xhlznaa'])[1]\")\n",
    "\n",
    "num_of_pages = (int(page_element.text)//12)+3 \n",
    "\n",
    "try:\n",
    "    try:\n",
    "        scroll_container = browser.find_element(\n",
    "            By.XPATH, \"(//div[@class='_15gu4wr'])[3]\")\n",
    "    except NoSuchElementException:\n",
    "        scroll_container = browser.find_element(\n",
    "            By.XPATH, \"(//div[@class='_15gu4wr'])[2]\")\n",
    "    finally:\n",
    "        \n",
    "        for page in range(1, num_of_pages):\n",
    "            with open(MARKUP_FILENAME, 'w', encoding='utf-8') as f:\n",
    "                f.write(browser.page_source)\n",
    "       \n",
    "            data_handler(page)\n",
    "            WebDriverWait(browser, 10).until(\n",
    "                EC.presence_of_element_located((By.XPATH, \"//div[@class='_5ocwns']//div[2]\"))\n",
    "            )\n",
    "\n",
    "            scroll_container = browser.find_element(\n",
    "                By.XPATH, \"(//div[@class='_15gu4wr'])[3]\"\n",
    "            )\n",
    "\n",
    "            browser.execute_script(\"arguments[0].scrollIntoView(false);\", scroll_container)\n",
    "\n",
    "            WebDriverWait(browser, 30).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, \"//div[@class='_5ocwns']//div[2]\"))\n",
    "            )\n",
    "\n",
    "            print(f\"Обработка страницы {page} завершена. Переход на следующую страницу.\")\n",
    "\n",
    "            next_page_button = WebDriverWait(browser, 10).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, \"//div[@class='_5ocwns']//div[2]\"))\n",
    "            )\n",
    "            next_page_button.click()\n",
    "            print(f\"Перешел на страницу {page + 1}\")\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "except IndexError:  \n",
    "    print(f'Total pages parsed {page-1}')\n",
    "    time.sleep(3)\n",
    "finally:\n",
    "    time.sleep(2)\n",
    "    browser.quit()  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
